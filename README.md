
import logging
import io
import os
import random
import matplotlib.pyplot as plt
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, InputFile
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext, CallbackQueryHandler

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© ---
# ÙŠÙ‚Ø±Ø£ Ø§Ù„ØªÙˆÙƒÙ† Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø£Ù…Ø§Ù†
TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")

# Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„ ---
# ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ± Ù„ÙŠØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ø§Ù„Ø®ÙˆØ§Ø¯Ù… Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ© (Ù…Ø«Ù„ Render Free Tier)
AI_DETECTOR_MODEL = 'distilroberta-base-openai-detector'
ai_detector_pipeline = None

def load_models():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©."""
    global ai_detector_pipeline
    if ai_detector_pipeline is None:
        try:
            from transformers import pipeline
            logger.info(f"Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒØ´Ù: {AI_DETECTOR_MODEL}...")
            ai_detector_pipeline = pipeline('text-classification', model=AI_DETECTOR_MODEL)
            logger.info("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­.")
        except ImportError:
            logger.error("Ù…ÙƒØªØ¨Ø© 'transformers' ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. Ø®Ø¯Ù…Ø© Ø§Ù„ÙƒØ´Ù Ù„Ù† ØªØ¹Ù…Ù„.")
        except Exception as e:
            logger.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒØ´Ù (Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø´ÙƒÙ„Ø© Ø°Ø§ÙƒØ±Ø©): {e}")

# --- Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---

def latex_to_image_bytes(latex_string: str):
    """ØªØ­ÙˆÙŠÙ„ Ø³Ù„Ø³Ù„Ø© LaTeX Ø¥Ù„Ù‰ ØµÙˆØ±Ø© PNG ÙƒÙ€ bytes."""
    try:
        full_latex_str = f"${latex_string}$"
        fig, ax = plt.subplots(figsize=(5, 1), dpi=300)
        ax.axis('off')
        ax.text(0.5, 0.5, full_latex_str, size=15, ha='center', va='center')
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.2, transparent=True)
        plt.close(fig)
        buf.seek(0)
        return buf.getvalue()
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ LaTeX: {e}")
        return None

def check_ai_text(text_to_check: str):
    """ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬Ø§Ù‹ Ù…Ø¯Ø±Ø¨Ø§Ù‹ Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Øµ Ù…ÙˆÙ„Ø¯Ø§Ù‹ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."""
    load_models() # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    if not ai_detector_pipeline:
        return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø®Ø¯Ù…Ø© ÙƒØ´Ù Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ Ø¨Ø³Ø¨Ø¨ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬."
    try:
        results = ai_detector_pipeline(text_to_check)
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù…ÙŠØ© (label)
        # 'LABEL_1' Ø£Ùˆ 'Real' Ø¹Ø§Ø¯Ø©Ù‹ Ù„Ù„Ù†Øµ Ø§Ù„Ø¨Ø´Ø±ÙŠØŒ 'LABEL_0' Ø£Ùˆ 'Fake' Ù„Ù„Ø¢Ù„ÙŠ
        ai_score = results[0]['score'] if results[0]['label'].upper() in ['FAKE', 'LABEL_0'] else 1 - results[0]['score']
        
        if ai_score > 0.8:
            return f"ğŸš¨ **ØªÙ… Ø§Ù„ÙƒØ´Ù Ø¨Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ© ({ai_score:.0%}) Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ù…ÙˆÙ„Ù‘Ø¯ Ø¨ÙˆØ§Ø³Ø·Ø© AI.**"
        elif ai_score > 0.5:
            return f"âš ï¸ **Ù‡Ù†Ø§Ùƒ Ø§Ø­ØªÙ…Ø§Ù„ ({ai_score:.0%}) Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ù…ÙˆÙ„Ù‘Ø¯ Ø¨ÙˆØ§Ø³Ø·Ø© AI.**"
        else:
            return f"âœ… **Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¬Ø­ØŒ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ù…ÙƒØªÙˆØ¨ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¥Ù†Ø³Ø§Ù†.** (Ø§Ø­ØªÙ…Ø§Ù„ AI: {ai_score:.0%})"
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ù†Øµ: {e}")
        return "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ."

def humanize_text(ai_text: str) -> str:
    """ØªØ¹Ø¯ÙŠÙ„ Ù†Øµ Ù…ÙˆÙ„Ù‘Ø¯ Ø¢Ù„ÙŠØ§Ù‹ Ù„ÙŠØ¨Ø¯Ùˆ Ø£ÙƒØ«Ø± Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¨Ø´Ø±ÙŠØ©."""
    sentences = ai_text.split('. ')
    new_sentences = []
    common_ai_phrases = ["ÙÙŠ Ø§Ù„Ø®ØªØ§Ù…ØŒ", "ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙˆÙ„ Ø£Ù†", "Ù…Ù† Ù†Ø§Ø­ÙŠØ© Ø£Ø®Ø±Ù‰ØŒ", "Ø¹Ù„Ø§ÙˆØ© Ø¹Ù„Ù‰ Ø°Ù„ÙƒØŒ", "ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø·Ø§ÙØŒ"]
    for i, sentence in enumerate(sentences):
        for phrase in common_ai_phrases:
            sentence = sentence.replace(phrase, "").strip()
        if random.random() < 0.15: # ØªÙ‚Ù„ÙŠÙ„ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            prefix = random.choice(["ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ØŒ ", "ÙÙŠ Ø±Ø£ÙŠÙŠØŒ ", "Ø¨ØµØ±Ø§Ø­Ø©ØŒ "])
            sentence = prefix + sentence[0].lower() + sentence[1:] if sentence else ""
        new_sentences.append(sentence.strip())
    humanized_output = ". ".join(filter(None, new_sentences))
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£ÙˆÙ„ ÙƒØ¨ÙŠØ±
    if humanized_output:
        humanized_output = humanized_output[0].upper() + humanized_output[1:]
    return ' '.join(humanized_output.split())

# --- Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø¨ÙˆØª ---

async def start(update: Update, context: CallbackContext) -> None:
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨ÙŠØ© Ù…Ø¹ Ø§Ù„Ø£Ø²Ø±Ø§Ø±."""
    keyboard = [
        [InlineKeyboardButton("ğŸ” ÙƒØ´Ù Ù†Øµ AI", callback_data='detect_ai')],
        [InlineKeyboardButton("âœï¸ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ø¨Ø´Ø±ÙŠ", callback_data='humanize_text')],
        [InlineKeyboardButton("ğŸ–¼ï¸ ØªØ­ÙˆÙŠÙ„ LaTeX Ù„ØµÙˆØ±Ø©", callback_data='latex_to_image')],
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    welcome_message = "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¨ÙˆØª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù†ØµÙˆØµ!\n\nØ§Ø®ØªØ± Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø£Ø¯Ù†Ø§Ù‡:"
    
    # ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ ØªØ¹Ø¯ÙŠÙ„ Ø±Ø³Ø§Ù„Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©
    if update.callback_query:
        await update.callback_query.edit_message_text(welcome_message, reply_markup=reply_markup, parse_mode='Markdown')
    else:
        await update.message.reply_text(welcome_message, reply_markup=reply_markup, parse_mode='Markdown')

async def button(update: Update, context: CallbackContext) -> None:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø²Ø±Ø§Ø±."""
    query = update.callback_query
    await query.answer()
    context.user_data['choice'] = query.data
    
    prompts = {
        'detect_ai': "Ø§Ù„Ø¢Ù†ØŒ Ø£Ø±Ø³Ù„ Ù„ÙŠ Ø§Ù„Ù†Øµ Ø£Ùˆ Ù…Ù„Ù `.txt` Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ ÙØ­ØµÙ‡.",
        'humanize_text': "Ø§Ù„Ø¢Ù†ØŒ Ø£Ø±Ø³Ù„ Ù„ÙŠ Ø§Ù„Ù†Øµ Ø£Ùˆ Ù…Ù„Ù `.txt` Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ 'Ø£Ù†Ø³Ù†ØªÙ‡'.",
        'latex_to_image': "Ø§Ù„Ø¢Ù†ØŒ Ø£Ø±Ø³Ù„ Ù„ÙŠ Ù…Ø¹Ø§Ø¯Ù„Ø© LaTeX (Ø¨Ø¯ÙˆÙ† $).\nÙ…Ø«Ø§Ù„: `\\frac{a^2}{b_i}`"
    }
    await query.edit_message_text(text=prompts[query.data], parse_mode='Markdown')

async def handle_text_or_file(update: Update, context: CallbackContext) -> None:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù†ØµÙŠØ© ÙˆØ§Ù„Ù…Ù„ÙØ§Øª."""
    if 'choice' not in context.user_data:
        await update.message.reply_text("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø®ØªÙŠØ§Ø± Ø®Ø¯Ù…Ø© Ø£ÙˆÙ„Ø§Ù‹ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø£Ù…Ø± /start.")
        return

    user_choice = context.user_data['choice']
    user_text = ""

    if update.message.text:
        user_text = update.message.text
    elif update.message.document:
        if update.message.document.mime_type == 'text/plain':
            file = await update.message.document.get_file()
            file_bytes = await file.download_as_bytearray()
            user_text = file_bytes.decode('utf-8')
        else:
            await update.message.reply_text("Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù Ù†ØµÙŠ (`.txt`).")
            return
    
    if not user_text:
        return

    processing_message = await update.message.reply_text("â³ ...Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±...", parse_mode='Markdown')
    
    try:
        if user_choice == 'detect_ai':
            result = check_ai_text(user_text)
            await processing_message.edit_text(result, parse_mode='Markdown')
        elif user_choice == 'humanize_text':
            result = humanize_text(user_text)
            await processing_message.edit_text(result)
        elif user_choice == 'latex_to_image':
            image_bytes = latex_to_image_bytes(user_text)
            if image_bytes:
                await update.message.reply_photo(photo=image_bytes, caption=f"ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©:\n`{user_text}`", parse_mode='Markdown')
                await processing_message.delete()
            else:
                await processing_message.edit_text("Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©. ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© ØµÙŠØºØ© LaTeX.")
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")
        await processing_message.edit_text("Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")

    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø© ÙˆØ§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    if 'choice' in context.user_data:
        del context.user_data['choice']
    await start(update, context)

def main() -> None:
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª."""
    if not TOKEN:
        logger.critical("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªÙˆÙƒÙ† Ø§Ù„Ø¨ÙˆØª! Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© TELEGRAM_BOT_TOKEN ÙÙŠ Ù…Ù†ØµØ© Ø§Ù„Ø§Ø³ØªØ¶Ø§ÙØ©.")
        return
        
    logger.info("Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª...")
    application = Application.builder().token(TOKEN).build()
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CallbackQueryHandler(button))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text_or_file))
    application.add_handler(MessageHandler(filters.Document.MimeType('text/plain'), handle_text_or_file))
    
    # Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
    load_models()

    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª
    application.run_polling()
    logger.info("ØªÙˆÙ‚Ù Ø§Ù„Ø¨ÙˆØª.")

if __name__ == '__main__':
    main()
    
